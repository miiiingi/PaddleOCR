import cv2
import numpy as np
import openvino as ov
from typing import List, Tuple
import argparse
import os
from db_postprocess import DBPostProcess


class OpenVINOOCR:
    def __init__(
        self,
        det_model_path: str,
        rec_model_path: str,
        device: str = "AUTO",  # "CPU", "GPU", "AUTO"
        visualize: bool = False,
    ):
        self.visualize = visualize
        self.filename = ""
        self.core = ov.Core()

        # ------------------------
        # Detection Model
        # ------------------------
        self.det_model = self.core.read_model(det_model_path)
        self.det_compiled = self.core.compile_model(self.det_model, device)
        self.det_input_layer = self.det_compiled.input(0)
        self.det_output_layer = self.det_compiled.output(0)

        # ------------------------
        # Recognition Model
        # ------------------------
        self.rec_model = self.core.read_model(rec_model_path)
        self.rec_compiled = self.core.compile_model(self.rec_model, device)
        self.rec_input_layer = self.rec_compiled.input(0)
        self.rec_output_layer = self.rec_compiled.output(0)

        self.db_postprocess = DBPostProcess(
            thresh=0.3,
            box_thresh=0.6,
            max_candidates=1000,
            unclip_ratio=1.5,
            box_type="quad",
        )

        print("âœ… OpenVINO models loaded successfully")

    def load_character_dict(self, dict_path):
        with open(dict_path, "r", encoding="utf-8") as f:
            chars = [line.strip() for line in f.readlines()]
        chars = [""] + chars  # blank for CTC
        return chars

    # =====================================================
    # DETECTION
    # =====================================================
    def preprocess_det(self, image: np.ndarray) -> Tuple[np.ndarray, float, Tuple]:
        """Detection ì „ì²˜ë¦¬ (ì¢Œí‘œ ì–´ê¸‹ë‚¨ ìˆ˜ì • ë²„ì „)"""
        h, w = image.shape[:2]

        target_size = 960

        ratio_h, ratio_w = target_size / h, target_size / w
        # 1ï¸âƒ£ ì‹¤ì œ resize í¬ê¸°
        resize_h = int(h * ratio_h)
        resize_w = int(w * ratio_w)
        resized = cv2.resize(image, (resize_w, resize_h))

        # 2ï¸âƒ£ 32ì˜ ë°°ìˆ˜ë¡œ ì˜¬ë¦¼ (ë‚´ë¦¼ âŒ)
        pad_h = int(np.ceil(resize_h / 32) * 32)
        pad_w = int(np.ceil(resize_w / 32) * 32)
        # 3ï¸âƒ£ padding (ì˜¤ë¥¸ìª½, ì•„ë˜ìª½ë§Œ)
        padded = np.zeros((pad_h, pad_w, 3), dtype=np.float32)
        padded[:resize_h, :resize_w, :] = resized.astype(np.float32)

        # 4ï¸âƒ£ ì •ê·œí™”
        mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)
        std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)
        normalized = (padded / 255.0 - mean) / std

        # 5ï¸âƒ£ CHW ë³€í™˜
        img_tensor = normalized.transpose(2, 0, 1)
        img_tensor = np.expand_dims(img_tensor, axis=0).astype(np.float32)
        return img_tensor, ratio_h, ratio_w, (h, w)

    def postprocess_det(self, det_map, thresh: float = 0.3):
        binary = (det_map > thresh).astype(np.uint8) * 255

        contours, _ = cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)

        boxes = []
        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)
            boxes.append(np.array([x, y, x + w, y + h]))

        return boxes

    def detect(self, image: np.ndarray) -> np.ndarray:
        input_tensor, ratio_h, ratio_w, original_size = self.preprocess_det(image)

        result = self.det_compiled([input_tensor])
        output = result[self.det_output_layer]  # output shape: (1,1,H,W)
        src_h, src_w = original_size
        shape_list = [[src_h, src_w, ratio_h, ratio_w]]

        outs_dict = {"maps": output}

        post_result = self.db_postprocess(outs_dict, shape_list)

        boxes = post_result[0]["points"]

        if self.visualize:
            self.draw_detect_results(image, boxes)

        return boxes

    # =====================================================
    # RECOGNITION
    # =====================================================
    def get_rotate_crop_image(self, img, points):
        """
        4ê°œì˜ ì (x, y)ìœ¼ë¡œ ëœ ë°•ìŠ¤ë¥¼ ë°›ì•„ ìˆ˜í‰ìœ¼ë¡œ í´ì„œ(Warp) ì˜ë¼ë‚´ëŠ” í•¨ìˆ˜
        points shape: (4, 2)
        """
        # 1. ì ì˜ ìˆœì„œë¥¼ ì •ë ¬ (ì¢Œìƒ, ìš°ìƒ, ìš°í•˜, ì¢Œí•˜ ìˆœì„œë¡œ ë³´ì •)
        # xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_p = points[np.argsort(points[:, 0])]

        # ê°€ì¥ ì™¼ìª½ 2ê°œ ì  ì¤‘ yê°€ ì‘ì€ê²Œ ì¢Œìƒ(TL), í°ê²Œ ì¢Œí•˜(BL)
        left_half = sorted_p[:2]
        left_half = left_half[np.argsort(left_half[:, 1])]
        tl, bl = left_half

        # ê°€ì¥ ì˜¤ë¥¸ìª½ 2ê°œ ì  ì¤‘ yê°€ ì‘ì€ê²Œ ìš°ìƒ(TR), í°ê²Œ ìš°í•˜(BR)
        right_half = sorted_p[2:]
        right_half = right_half[np.argsort(right_half[:, 1])]
        tr, br = right_half

        # ì •ë ¬ëœ ì¢Œí‘œ
        img_crop_width = int(max(np.linalg.norm(tr - tl), np.linalg.norm(br - bl)))
        img_crop_height = int(max(np.linalg.norm(tl - bl), np.linalg.norm(tr - br)))

        pts_std = np.float32(
            [
                [0, 0],
                [img_crop_width, 0],
                [img_crop_width, img_crop_height],
                [0, img_crop_height],
            ]
        )

        # ì†ŒìŠ¤ ì¢Œí‘œ (float32ë¡œ ë³€í™˜ í•„ìˆ˜)
        src_pts = np.float32([tl, tr, br, bl])

        # íˆ¬ì‹œ ë³€í™˜ í–‰ë ¬ ê³„ì‚° ë° ì ìš©
        M = cv2.getPerspectiveTransform(src_pts, pts_std)
        dst_img = cv2.warpPerspective(
            img,
            M,
            (img_crop_width, img_crop_height),
            borderMode=cv2.BORDER_REPLICATE,
            flags=cv2.INTER_CUBIC,
        )

        # ì„¸ë¡œë¡œ ê¸´ ì´ë¯¸ì§€(ì„¸ë¡œì“°ê¸° ë“±)ì¼ ê²½ìš° íšŒì „ (PP-OCR ë¡œì§ìƒ ê°€ë¡œë¡œ ëˆ•í˜€ì•¼ í•¨)
        if dst_img.shape[0] / dst_img.shape[1] > 1.5:
            dst_img = np.rot90(dst_img, k=1)

        return dst_img

    def preprocess_rec(self, image: np.ndarray, boxes: np.ndarray) -> List[np.ndarray]:
        """
        PP-OCRv5 Rec ëª¨ë¸ ì „ì²˜ë¦¬ (Numpy array boxes ëŒ€ì‘ ë²„ì „)
        boxes shape: (N, 4, 2)
        """
        img_h = 48
        img_w = 320

        norm_img_batch = []

        # boxesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ numpy arrayì´ë¯€ë¡œ ë°”ë¡œ ìˆœíšŒ
        for box in boxes:
            # box shape: (4, 2) -> [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]

            # 1. Perspective Transformìœ¼ë¡œ í…ìŠ¤íŠ¸ ì˜ì—­ì„ ë˜‘ë°”ë¡œ í´ì„œ ìë¦„
            crop = self.get_rotate_crop_image(image, box)

            if crop.size == 0:
                crop = np.zeros((img_h, img_w, 3), dtype=np.uint8)

            # 2. ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ë†’ì´ 48 ê³ ì •)
            h, w = crop.shape[:2]
            ratio = w / float(h)

            new_w = int(img_h * ratio)
            if new_w > img_w:
                new_w = img_w

            resized = cv2.resize(crop, (new_w, img_h))

            # 3. ì •ê·œí™”: (Pixel - 0.5) / 0.5 => [-1, 1]
            resized = resized.astype(np.float32) / 255.0
            resized = (resized - 0.5) / 0.5

            # 4. Padding (ì˜¤ë¥¸ìª½ ì±„ìš°ê¸°)
            padded = np.zeros((3, img_h, img_w), dtype=np.float32)
            resized = resized.transpose(2, 0, 1)  # HWC -> CHW
            padded[:, :, :new_w] = resized

            norm_img_batch.append(padded)

        return norm_img_batch

    def recognize(self, image: np.ndarray, boxes: List[np.ndarray]):

        crops = self.preprocess_rec(image, boxes)
        if len(crops) == 0:
            return []

        # ğŸ”¥ 1. ê°€ì¥ í° width ì°¾ê¸°
        max_w = max(crop.shape[2] for crop in crops)

        padded_crops = []

        for crop in crops:
            c, h, w = crop.shape

            if w < max_w:
                pad_width = max_w - w
                pad = np.zeros((c, h, pad_width), dtype=np.float32)
                crop = np.concatenate([crop, pad], axis=2)

            padded_crops.append(crop)

        # ğŸ”¥ 2. ì´ì œ ì•ˆì „í•˜ê²Œ batch ìƒì„±
        batch = np.stack(padded_crops, axis=0)

        result = self.rec_compiled([batch])
        output = result[self.rec_output_layer]

        return output

    def ctc_decode(self, preds, character_dict):
        """
        preds: (N, T, C) - (9, 40, 18385)
        character_dict: list (ë¡œë“œëœ ì‚¬ì „)
        """
        texts = []
        confs = []

        # ì˜ˆì¸¡ê°’ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤ì™€ ê·¸ í™•ë¥ ê°’ì„ ê°€ì ¸ì˜´
        preds_idx = preds.argmax(axis=2)  # (N, T)
        preds_prob = preds.max(axis=2)  # (N, T)

        for pred_idx, pred_prob in zip(preds_idx, preds_prob):
            char_list = []
            conf_list = []
            prev_idx = 0  # CTCì˜ blank indexëŠ” ë³´í†µ 0ì…ë‹ˆë‹¤.

            for i in range(len(pred_idx)):
                idx = int(pred_idx[i])
                prob = pred_prob[i]

                # 1. Blank(0)ê°€ ì•„ë‹ˆê³ , ì´ì „ ì¸ë±ìŠ¤ì™€ ì¤‘ë³µë˜ì§€ ì•Šì„ ë•Œë§Œ ì¶”ê°€
                if idx > 0 and (i == 0 or idx != pred_idx[i - 1]):
                    # character_dict ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ”ì§€ ì²´í¬ (ì•ˆì „ì¥ì¹˜)
                    if idx < len(character_dict):
                        char_list.append(character_dict[idx])
                        conf_list.append(prob)

            text = "".join(char_list)
            conf = np.mean(conf_list) if conf_list else 0.0

            texts.append(text)
            confs.append(float(conf))

        return texts, confs

    def draw_ocr_results(self, image, boxes, texts, confs):
        vis_img = image.copy()

        for box, text, conf in zip(boxes, texts, confs):
            # (N, 4, 2)ì—ì„œ í•œ ë°•ìŠ¤(4, 2)ë¥¼ ê°€ì ¸ì™€ ì •ìˆ˜í˜• ë³€í™˜
            pts = box.astype(np.int32)

            # 1. ë‹¤ê°í˜•(Polylines) ê·¸ë¦¬ê¸°
            # ptsëŠ” (4, 1, 2) í˜•íƒœì—¬ì•¼ cv2.polylinesì—ì„œ ì¸ì‹í•¨
            cv2.polylines(vis_img, [pts.reshape((-1, 1, 2))], True, (0, 255, 0), 2)

            # 2. í…ìŠ¤íŠ¸ ìœ„ì¹˜ (ë°•ìŠ¤ ì ë“¤ ì¤‘ ê°€ì¥ ìœ„ìª½/ì™¼ìª½ ê¸°ì¤€)
            min_x = int(np.min(box[:, 0]))
            min_y = int(np.min(box[:, 1]))

            label = f"{text} ({conf:.2f})"
            (tw, th), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
            )

            # 3. í…ìŠ¤íŠ¸ ë°°ê²½ ì‚¬ê°í˜•
            cv2.rectangle(
                vis_img, (min_x, min_y - th - 5), (min_x + tw, min_y), (0, 255, 0), -1
            )

            # 4. í…ìŠ¤íŠ¸ ì“°ê¸°
            cv2.putText(
                vis_img,
                label,
                (min_x, min_y - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 0, 0),
                1,
                cv2.LINE_AA,
            )
        save_path = "detect_rec_result_" + self.filename

        cv2.imwrite(save_path, vis_img)
        print(f"âœ… debug(detect_rec) image saved: {save_path}")

    def draw_detect_results(self, image, boxes):
        debug_img = image.copy()

        for i, box in enumerate(boxes):

            box = box.astype(np.int32)

            # ë‹¤ê°í˜• ê·¸ë¦¬ê¸°
            cv2.polylines(
                debug_img,
                [box],
                isClosed=True,
                color=(0, 255, 0),
                thickness=2,
            )

            # ì¸ë±ìŠ¤ í‘œì‹œ
            cv2.putText(
                debug_img,
                str(i),
                tuple(box[0]),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (0, 0, 255),
                2,
            )
        save_path = "detect_result_" + self.filename
        cv2.imwrite(save_path, debug_img)
        print(f"âœ… debug(detect) image saved: {save_path}")

    # =====================================================
    # FULL PIPELINE
    # =====================================================
    def predict(self, image_path: str):
        image = cv2.imread(image_path)
        character_dict = self.load_character_dict(r"ppocr/utils/dict/ppocrv5_dict.txt")

        # 1ï¸âƒ£ Detect
        boxes = self.detect(image)

        # 2ï¸âƒ£ Recognize
        rec_output = self.recognize(image, boxes)
        print(f"rec_output shape: {rec_output.shape}")

        text, confs = self.ctc_decode(rec_output, character_dict)

        self.draw_ocr_results(image, boxes, text, confs)

        return boxes, rec_output


# =====================================================
# ì‹¤í–‰ ì˜ˆì‹œ
# =====================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ONNX â†’ OpenVINO Converter")

    parser.add_argument(
        "--det_model_path",
        type=str,
        required=True,
        help="Input Detection model path (.xml)",
    )
    parser.add_argument(
        "--rec_model_path",
        type=str,
        required=True,
        help="Input Recognition model path (.xml)",
    )
    parser.add_argument(
        "--visualize",
        action="store_true",
        help="Visualize detection results",
    )

    args = parser.parse_args()

    ocr = OpenVINOOCR(
        args.det_model_path, args.rec_model_path, device="CPU", visualize=args.visualize
    )
    folder = r"/mnt/d/workspace/HENKEL/syringe_temp"
    for filename in os.listdir(folder):
        if not filename.lower().endswith((".png", ".jpg", ".jpeg")):
            continue

        ocr.filename = filename
        image_path = os.path.join(folder, filename)
        print(f"\n{'='*60}")
        print(f"Processing: {image_path}")
        print(f"{'='*60}")

        boxes, rec_out = ocr.predict(image_path)

        print("Detected boxes:", len(boxes))
        print("Recognition output shape:", rec_out.shape if len(boxes) > 0 else None)
