import os
import time
import re
import cv2
import numpy as np
import tensorrt as trt
import pycuda.driver as cuda
from typing import List, Tuple, Dict
import torch
from db_postprocess import DBPostProcess
import argparse
import tqdm

cuda.init()
device = cuda.Device(0)

ctx = device.retain_primary_context()
ctx.push()


class PaddleOCRTensorRT:
    """PaddleOCR TensorRT Inference Engine"""

    def __init__(
        self,
        det_engine_path: str,
        rec_engine_path: str,
        visualize: bool = False,
    ):
        """
        Args:
            det_engine_path: Detection TensorRT engine íŒŒì¼ ê²½ë¡œ
            rec_engine_path: Recognition TensorRT engine íŒŒì¼ ê²½ë¡œ
            dict_path: OCR ë¬¸ì ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
        """
        self.visualize = visualize
        self.filename = ""
        # TensorRT ë¡œê±°
        self.logger = trt.Logger(trt.Logger.INFO)

        # Detection Engine ë¡œë“œ
        self.det_engine, self.det_context = self._load_engine(det_engine_path)

        # Recognition Engine ë¡œë“œ
        self.rec_engine, self.rec_context = self._load_engine(rec_engine_path)

        # ë¬¸ì ì‚¬ì „ ë¡œë“œ
        self.db_postprocess = DBPostProcess(
            thresh=0.3,
            box_thresh=0.6,
            max_candidates=1000,
            unclip_ratio=1.5,
            box_type="quad",
        )

        print(f"âœ… Detection engine loaded: {det_engine_path}")
        print(f"âœ… Recognition engine loaded: {rec_engine_path}")

    def _load_engine(
        self, engine_path: str
    ) -> Tuple[trt.ICudaEngine, trt.IExecutionContext]:
        """TensorRT ì—”ì§„ ë¡œë“œ"""
        with open(engine_path, "rb") as f:
            engine_data = f.read()

        runtime = trt.Runtime(self.logger)
        engine = runtime.deserialize_cuda_engine(engine_data)
        context = engine.create_execution_context()

        return engine, context

    def _load_character_dict(self, dict_path):
        with open(dict_path, "r", encoding="utf-8") as f:
            chars = [line.strip() for line in f.readlines()]
        chars = [""] + chars  # blank for CTC
        return chars

    def _allocate_buffers(
        self,
        engine: trt.ICudaEngine,
        context: trt.IExecutionContext,
        N: int,
    ) -> Tuple[Dict, cuda.Stream, Dict]:
        """ë²„í¼ í• ë‹¹ - ë™ì  shape ê¸°ë°˜"""
        bindings = {}
        stream = cuda.Stream()
        output_tensors = {}

        for i in range(engine.num_io_tensors):
            binding = engine.get_tensor_name(i)

            # âš ï¸ contextì—ì„œ ì‹¤ì œ ì„¤ì •ëœ shape ê°€ì ¸ì˜¤ê¸° (ì¤‘ìš”!)
            shape = context.get_tensor_shape(binding)
            dtype = engine.get_tensor_dtype(binding)

            # -1ì´ ìˆìœ¼ë©´ ì—ëŸ¬
            if -1 in shape:
                raise RuntimeError(
                    f"Shape not fully specified for {binding}: {shape}. "
                    "Make sure to call set_input_shape() first!"
                )

            np_dtype = trt.nptype(dtype)

            if engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:
                # Input ë²„í¼ - GPU ë©”ëª¨ë¦¬ë§Œ í• ë‹¹
                size = int(np.prod(shape)) * np.dtype(np_dtype).itemsize
                device_mem = cuda.mem_alloc(size)
                bindings[binding] = device_mem
            else:
                # Output ë²„í¼ - Torch í…ì„œë¡œ í• ë‹¹
                torch_dtype = torch.from_numpy(np.array([], dtype=np_dtype)).dtype
                output_tensor = torch.empty(
                    size=tuple(shape), dtype=torch_dtype, device="cuda"
                )
                output_tensors[binding] = output_tensor
                bindings[binding] = output_tensor.data_ptr()

        return bindings, stream, output_tensors

    def _do_inference(self, context: trt.IExecutionContext, stream: cuda.Stream):
        """ì¶”ë¡  ì‹¤í–‰"""
        context.execute_async_v3(stream_handle=stream.handle)
        stream.synchronize()

    def _preprocess_det(self, image: np.ndarray) -> Tuple[np.ndarray, float, Tuple]:
        """Detection ì „ì²˜ë¦¬ (ì¢Œí‘œ ì–´ê¸‹ë‚¨ ìˆ˜ì • ë²„ì „)"""
        h, w = image.shape[:2]
        target_size = 960
        ratio_h, ratio_w = target_size / h, target_size / w
        # 1ï¸âƒ£ ì‹¤ì œ resize í¬ê¸°
        resize_h = int(h * ratio_h)
        resize_w = int(w * ratio_w)
        resized = cv2.resize(image, (resize_w, resize_h))
        # 2ï¸âƒ£ 32ì˜ ë°°ìˆ˜ë¡œ ì˜¬ë¦¼ (ë‚´ë¦¼ âŒ)
        pad_h = int(np.ceil(resize_h / 32) * 32)
        pad_w = int(np.ceil(resize_w / 32) * 32)
        # 3ï¸âƒ£ padding (ì˜¤ë¥¸ìª½, ì•„ë˜ìª½ë§Œ)
        padded = np.zeros((pad_h, pad_w, 3), dtype=np.float32)
        padded[:resize_h, :resize_w, :] = resized.astype(np.float32)
        # 4ï¸âƒ£ ì •ê·œí™”
        mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)
        std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)
        normalized = (padded / 255.0 - mean) / std
        # 5ï¸âƒ£ CHW ë³€í™˜
        img_tensor = normalized.transpose(2, 0, 1)
        img_tensor = np.expand_dims(img_tensor, axis=0).astype(np.float32)
        return img_tensor, ratio_h, ratio_w, (h, w)

    def get_rotate_crop_image(self, img, points):
        """
        4ê°œì˜ ì (x, y)ìœ¼ë¡œ ëœ ë°•ìŠ¤ë¥¼ ë°›ì•„ ìˆ˜í‰ìœ¼ë¡œ í´ì„œ(Warp) ì˜ë¼ë‚´ëŠ” í•¨ìˆ˜
        points shape: (4, 2)
        """
        # 1. ì ì˜ ìˆœì„œë¥¼ ì •ë ¬ (ì¢Œìƒ, ìš°ìƒ, ìš°í•˜, ì¢Œí•˜ ìˆœì„œë¡œ ë³´ì •)
        # xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_p = points[np.argsort(points[:, 0])]

        # ê°€ì¥ ì™¼ìª½ 2ê°œ ì  ì¤‘ yê°€ ì‘ì€ê²Œ ì¢Œìƒ(TL), í°ê²Œ ì¢Œí•˜(BL)
        left_half = sorted_p[:2]
        left_half = left_half[np.argsort(left_half[:, 1])]
        tl, bl = left_half

        # ê°€ì¥ ì˜¤ë¥¸ìª½ 2ê°œ ì  ì¤‘ yê°€ ì‘ì€ê²Œ ìš°ìƒ(TR), í°ê²Œ ìš°í•˜(BR)
        right_half = sorted_p[2:]
        right_half = right_half[np.argsort(right_half[:, 1])]
        tr, br = right_half

        # ì •ë ¬ëœ ì¢Œí‘œ
        img_crop_width = int(max(np.linalg.norm(tr - tl), np.linalg.norm(br - bl)))
        img_crop_height = int(max(np.linalg.norm(tl - bl), np.linalg.norm(tr - br)))

        pts_std = np.float32(
            [
                [0, 0],
                [img_crop_width, 0],
                [img_crop_width, img_crop_height],
                [0, img_crop_height],
            ]
        )

        # ì†ŒìŠ¤ ì¢Œí‘œ (float32ë¡œ ë³€í™˜ í•„ìˆ˜)
        src_pts = np.float32([tl, tr, br, bl])

        # íˆ¬ì‹œ ë³€í™˜ í–‰ë ¬ ê³„ì‚° ë° ì ìš©
        M = cv2.getPerspectiveTransform(src_pts, pts_std)
        dst_img = cv2.warpPerspective(
            img,
            M,
            (img_crop_width, img_crop_height),
            borderMode=cv2.BORDER_REPLICATE,
            flags=cv2.INTER_CUBIC,
        )

        # ì„¸ë¡œë¡œ ê¸´ ì´ë¯¸ì§€(ì„¸ë¡œì“°ê¸° ë“±)ì¼ ê²½ìš° íšŒì „ (PP-OCR ë¡œì§ìƒ ê°€ë¡œë¡œ ëˆ•í˜€ì•¼ í•¨)
        if dst_img.shape[0] / dst_img.shape[1] > 1.5:
            dst_img = np.rot90(dst_img, k=1)

        return dst_img

    def _preprocess_rec(self, image: np.ndarray, boxes: np.ndarray) -> List[np.ndarray]:
        """
        PP-OCRv5 Rec ëª¨ë¸ ì „ì²˜ë¦¬ (Numpy array boxes ëŒ€ì‘ ë²„ì „)
        boxes shape: (N, 4, 2)
        """
        img_h = 48
        img_w = 320

        norm_img_batch = []

        # boxesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ numpy arrayì´ë¯€ë¡œ ë°”ë¡œ ìˆœíšŒ
        for box in boxes:
            # box shape: (4, 2) -> [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]

            # 1. Perspective Transformìœ¼ë¡œ í…ìŠ¤íŠ¸ ì˜ì—­ì„ ë˜‘ë°”ë¡œ í´ì„œ ìë¦„
            crop = self.get_rotate_crop_image(image, box)

            if crop.size == 0:
                crop = np.zeros((img_h, img_w, 3), dtype=np.uint8)

            # 2. ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ë†’ì´ 48 ê³ ì •)
            h, w = crop.shape[:2]
            ratio = w / float(h)

            new_w = int(img_h * ratio)
            if new_w > img_w:
                new_w = img_w

            resized = cv2.resize(crop, (new_w, img_h))

            # 3. ì •ê·œí™”: (Pixel - 0.5) / 0.5 => [-1, 1]
            resized = resized.astype(np.float32) / 255.0
            resized = (resized - 0.5) / 0.5

            # 4. Padding (ì˜¤ë¥¸ìª½ ì±„ìš°ê¸°)
            padded = np.zeros((3, img_h, img_w), dtype=np.float32)
            resized = resized.transpose(2, 0, 1)  # HWC -> CHW
            padded[:, :, :new_w] = resized

            norm_img_batch.append(padded)

        return norm_img_batch

    def _postprocess_rec(
        self, preds: np.ndarray, boxes: List[np.ndarray]
    ) -> List[Tuple[str, float, np.ndarray]]:
        """Recognition í›„ì²˜ë¦¬"""
        results = []

        for i, box in enumerate(boxes):
            pred = preds[i]  # (T, C)

            # CTC ë””ì½”ë”©
            indices = np.argmax(pred, axis=-1)

            # ì¤‘ë³µ ì œê±° ë° blank ì œê±°
            text = []
            prev_idx = -1
            for idx in indices:
                if idx != prev_idx and idx != 0:  # 0ì€ blank
                    if self.char_dict and idx < len(self.char_dict):
                        text.append(self.char_dict[idx])
                prev_idx = idx

            text_str = "".join(text)

            # ì‹ ë¢°ë„ ê³„ì‚°
            max_probs = np.max(pred, axis=-1)
            confidence = float(np.mean(max_probs))

            results.append((text_str, confidence, box))

        return results

    def detect(self, image: np.ndarray):

        img_tensor, ratio_h, ratio_w, orig_size = self._preprocess_det(image)
        img_tensor = np.ascontiguousarray(img_tensor)
        img_torch = torch.from_numpy(img_tensor).cuda()

        input_name = None
        for i in range(self.det_engine.num_io_tensors):
            name = self.det_engine.get_tensor_name(i)
            if self.det_engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                input_name = name
                break

        N, C, H, W = img_tensor.shape
        self.det_context.set_input_shape(input_name, (N, C, H, W))

        bindings, stream, outputs = self._allocate_buffers(
            self.det_engine, self.det_context, N
        )

        for i in range(self.det_engine.num_io_tensors):
            binding = self.det_engine.get_tensor_name(i)
            self.det_context.set_tensor_address(binding, int(bindings[binding]))

        cuda.memcpy_dtod_async(
            dest=int(bindings[input_name]),
            src=int(img_torch.data_ptr()),
            size=img_torch.nbytes,
            stream=stream,
        )

        self._do_inference(self.det_context, stream)

        output_name = list(outputs.keys())[0]
        pred = outputs[output_name]  # torch cuda tensor (1,1,H,W)

        # ğŸ”¥ Paddle DBPostProcess ì ìš©
        pred_cpu = pred.detach().cpu().numpy()
        src_h, src_w = orig_size
        shape_list = [[src_h, src_w, ratio_h, ratio_w]]

        outs_dict = {"maps": pred_cpu}

        post_result = self.db_postprocess(outs_dict, shape_list)

        boxes = post_result[0]["points"]

        if self.visualize:
            self.draw_detect_results(image, boxes)

        return boxes

    def recognize(
        self, image: np.ndarray, boxes: List[np.ndarray]
    ) -> List[Tuple[str, float, np.ndarray]]:

        # ì „ì²˜ë¦¬
        crops = self._preprocess_rec(image, boxes)

        if len(crops) == 0:
            return []

        # ğŸ”¥ 1. ê°€ì¥ í° width ì°¾ê¸°
        max_w = max(crop.shape[2] for crop in crops)

        padded_crops = []

        for crop in crops:
            c, h, w = crop.shape

            if w < max_w:
                pad_width = max_w - w
                pad = np.zeros((c, h, pad_width), dtype=np.float32)
                crop = np.concatenate([crop, pad], axis=2)

            padded_crops.append(crop)

        # ğŸ”¥ 2. ì´ì œ ì•ˆì „í•˜ê²Œ batch ìƒì„±
        batch = np.stack(padded_crops, axis=0)

        # ì…ë ¥ shape ì„¤ì •
        input_name = None
        for i in range(self.rec_engine.num_io_tensors):
            name = self.rec_engine.get_tensor_name(i)
            if self.rec_engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                input_name = name
                break

        N, C, H, W = batch.shape
        self.rec_context.set_input_shape(input_name, (N, C, H, W))

        # ë²„í¼ í• ë‹¹
        bindings, stream, outputs = self._allocate_buffers(
            self.rec_engine, self.rec_context, N
        )

        # ì…ë ¥ ë°ì´í„° ë³µì‚¬
        for binding in self.rec_engine:
            self.rec_context.set_tensor_address(binding, int(bindings[binding]))

        img_torch = torch.from_numpy(batch).cuda()
        cuda.memcpy_dtod_async(
            dest=int(bindings[input_name]),
            src=int(img_torch.data_ptr()),
            size=img_torch.nbytes,
            stream=stream,
        )

        # ì¶”ë¡ 
        self._do_inference(self.rec_context, stream)

        # ì¶œë ¥ ê°€ì ¸ì˜¤ê¸°
        output_name = list(outputs.keys())[0]
        preds = outputs[output_name].cpu().numpy()

        return preds

    def ctc_decode(self, preds, character_dict):
        """
        preds: (N, T, C) - (9, 40, 18385)
        character_dict: list (ë¡œë“œëœ ì‚¬ì „)
        """
        texts = []
        confs = []

        # ì˜ˆì¸¡ê°’ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤ì™€ ê·¸ í™•ë¥ ê°’ì„ ê°€ì ¸ì˜´
        preds_idx = preds.argmax(axis=2)  # (N, T)
        preds_prob = preds.max(axis=2)  # (N, T)

        for pred_idx, pred_prob in zip(preds_idx, preds_prob):
            char_list = []
            conf_list = []
            prev_idx = 0  # CTCì˜ blank indexëŠ” ë³´í†µ 0ì…ë‹ˆë‹¤.

            for i in range(len(pred_idx)):
                idx = int(pred_idx[i])
                prob = pred_prob[i]

                # 1. Blank(0)ê°€ ì•„ë‹ˆê³ , ì´ì „ ì¸ë±ìŠ¤ì™€ ì¤‘ë³µë˜ì§€ ì•Šì„ ë•Œë§Œ ì¶”ê°€
                if idx > 0 and (i == 0 or idx != pred_idx[i - 1]):
                    # character_dict ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ”ì§€ ì²´í¬ (ì•ˆì „ì¥ì¹˜)
                    if idx < len(character_dict):
                        char_list.append(character_dict[idx])
                        conf_list.append(prob)

            text = "".join(char_list)
            conf = np.mean(conf_list) if conf_list else 0.0

            texts.append(text)
            confs.append(float(conf))

        return texts, confs

    def draw_ocr_results(self, image, boxes, texts, confs):
        vis_img = image.copy()

        for box, text, conf in zip(boxes, texts, confs):
            # (N, 4, 2)ì—ì„œ í•œ ë°•ìŠ¤(4, 2)ë¥¼ ê°€ì ¸ì™€ ì •ìˆ˜í˜• ë³€í™˜
            pts = box.astype(np.int32)

            # 1. ë‹¤ê°í˜•(Polylines) ê·¸ë¦¬ê¸°
            # ptsëŠ” (4, 1, 2) í˜•íƒœì—¬ì•¼ cv2.polylinesì—ì„œ ì¸ì‹í•¨
            cv2.polylines(vis_img, [pts.reshape((-1, 1, 2))], True, (0, 255, 0), 2)

            # 2. í…ìŠ¤íŠ¸ ìœ„ì¹˜ (ë°•ìŠ¤ ì ë“¤ ì¤‘ ê°€ì¥ ìœ„ìª½/ì™¼ìª½ ê¸°ì¤€)
            min_x = int(np.min(box[:, 0]))
            min_y = int(np.min(box[:, 1]))

            label = f"{text} ({conf:.2f})"
            (tw, th), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
            )

            # 3. í…ìŠ¤íŠ¸ ë°°ê²½ ì‚¬ê°í˜•
            cv2.rectangle(
                vis_img, (min_x, min_y - th - 5), (min_x + tw, min_y), (0, 255, 0), -1
            )

            # 4. í…ìŠ¤íŠ¸ ì“°ê¸°
            cv2.putText(
                vis_img,
                label,
                (min_x, min_y - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 0, 0),
                1,
                cv2.LINE_AA,
            )
        save_path = "trt_detect_rec_result_" + self.filename

        cv2.imwrite(save_path, vis_img)
        print(f"âœ… debug(detect_rec) image saved: {save_path}")

    def draw_detect_results(self, image, boxes):
        debug_img = image.copy()

        for i, box in enumerate(boxes):

            box = box.astype(np.int32)

            # ë‹¤ê°í˜• ê·¸ë¦¬ê¸°
            cv2.polylines(
                debug_img,
                [box],
                isClosed=True,
                color=(0, 255, 0),
                thickness=2,
            )

            # ì¸ë±ìŠ¤ í‘œì‹œ
            cv2.putText(
                debug_img,
                str(i),
                tuple(box[0]),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (0, 0, 255),
                2,
            )
        save_path = "trt_detect_result_" + self.filename
        cv2.imwrite(save_path, debug_img)
        print(f"âœ… debug(detect) image saved: {save_path}")

    def predict(self, image_path: str) -> List[Tuple[str, float, np.ndarray]]:
        """ì „ì²´ OCR íŒŒì´í”„ë¼ì¸"""
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Failed to load image: {image_path}")

        # 1. í…ìŠ¤íŠ¸ ê²€ì¶œ
        boxes = self.detect_text(image)
        exit(0)

        # 2. í…ìŠ¤íŠ¸ ì¸ì‹
        results = self.recognize_text(image, boxes)

        return results


def main():
    """ì‚¬ìš© ì˜ˆì œ"""
    # TensorRT ì—”ì§„ íŒŒì¼ ê²½ë¡œ
    det_engine_path = r"output_onnx/det.trt"
    rec_engine_path = r"output_onnx/rec.trt"
    dict_path = r"ppocr/utils/dict/ppocrv5_dict.txt"  # ì˜µì…˜

    # OCR ì—”ì§„ ì´ˆê¸°í™”
    ocr = PaddleOCRTensorRT(
        det_engine_path=det_engine_path,
        rec_engine_path=rec_engine_path,
        dict_path=dict_path,
    )

    # ì´ë¯¸ì§€ í´ë” ì²˜ë¦¬
    folder = r"/mnt/d/workspace/HENKEL/syringe_temp"

    # ì •ê·œì‹ íŒ¨í„´
    pattern = re.compile(
        r"(batch|batch number|number:|idh#|net weight|weight|no\.|P/N:|BATCH|"
        r"Syringe#:|EXP:|Net weight:|weight:|Storage Temp:|Temp:|IDH|#:|LOT#|"
        r"D.O.M:|SID#|Part No:|No:|Vender ID:|ID:|Lot No:|No:|Temp:|"
        r"Storage Temp:|DOM|NET WT:|WT:|P.O#|Hen)",
        re.IGNORECASE,
    )

    for filename in os.listdir(folder):
        if not filename.lower().endswith((".png", ".jpg", ".jpeg")):
            continue

        image_path = os.path.join(folder, filename)
        print(f"\n{'='*60}")
        print(f"Processing: {image_path}")
        print(f"{'='*60}")

        # ì¶”ë¡ 
        start = time.time()
        results = ocr.predict(image_path)
        end = time.time()

        # ê²°ê³¼ ì¶œë ¥
        for text, score, box in results:
            text_norm = text.lower().strip()
            if pattern.search(text):
                print(f"âœ… text: {text}")
                print(f"   score: {score:.3f}")
                print(f"   box: {box.tolist()}")

        print(f"\nâ±ï¸  Duration: {end - start:.3f}s")
        print(f"ğŸ“Š Total detections: {len(results)}")


if __name__ == "__main__":
    try:
        main()
    finally:
        ctx.pop()
